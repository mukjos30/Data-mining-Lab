{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f254b29a-abbd-440a-b835-28223ef9563c",
   "metadata": {},
   "source": [
    "Q4. Train & Test RSS - Linear/Nonlinear Models\n",
    "I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response.\n",
    "I then fit a linear regression model to the data: Y=β0^+β1^X+ϵ\n",
    "as well as a separate cubic regression: Y=β0^+β1^X+β2^X2+β3^X3+ϵ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c7a63-3ae0-4164-8885-3e90ca78d3d1",
   "metadata": {},
   "source": [
    "(a) Suppose that the true relationship between X and Y is linear,\n",
    "i.e. Y = #0 + #1X + \". Consider the training residual sum of\n",
    "squares (RSS) for the linear regression, and also the training\n",
    "RSS for the cubic regression. Would we expect one to be lower\n",
    "than the other, would we expect them to be the same, or is there\n",
    "not enough information to tell? Justify your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e3c283-3c96-40b4-b0bf-dd6586b38452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== (a) Linear True — TRAINING RSS ===\n",
      "Linear RSS: 396.98\n",
      "Cubic RSS : 388.46\n",
      "Justification:\n",
      "I would expect similar results, but with the cubic regression with a lower training RSS. We are basically increasing the model flexibility by going from linear → cubic, and we are working with a small sample of data, so I’d expect the cubic model to overfit to any nonlinearities and hence have a lower training RSS.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 100\n",
    "\n",
    "# Generate linear data\n",
    "X = np.random.uniform(-3, 3, n).reshape(-1, 1)\n",
    "y_linear = 2 + 3*X.flatten() + np.random.normal(0, 2, n)\n",
    "\n",
    "# Linear model\n",
    "lin = LinearRegression()\n",
    "lin.fit(X, y_linear)\n",
    "y_train_pred_lin = lin.predict(X)\n",
    "rss_train_lin = np.sum((y_linear - y_train_pred_lin)**2)\n",
    "\n",
    "# Cubic model\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_cubic = poly.fit_transform(X)\n",
    "cub = LinearRegression()\n",
    "cub.fit(X_cubic, y_linear)\n",
    "y_train_pred_cub = cub.predict(X_cubic)\n",
    "rss_train_cub = np.sum((y_linear - y_train_pred_cub)**2)\n",
    "\n",
    "print(\"=== (a) Linear True — TRAINING RSS ===\")\n",
    "print(f\"Linear RSS: {rss_train_lin:.2f}\")\n",
    "print(f\"Cubic RSS : {rss_train_cub:.2f}\")\n",
    "print(\"Justification:\")\n",
    "print(\"I would expect similar results, but with the cubic regression with a lower training RSS. \"\n",
    "      \"We are basically increasing the model flexibility by going from linear → cubic, \"\n",
    "      \"and we are working with a small sample of data, so I’d expect the cubic model to overfit \"\n",
    "      \"to any nonlinearities and hence have a lower training RSS.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cab771a-53d1-41b4-ac64-c573e170ce5a",
   "metadata": {},
   "source": [
    "(b) Answer (a) using test rather than training RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa807f89-2bc1-4524-95e2-cdcf0a315e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== (b) Linear True — TEST RSS ===\n",
      "Linear RSS: 104.02\n",
      "Cubic RSS : 111.65\n",
      "Justification:\n",
      "Here I would expect the linear regression to perform slightly better than the cubic regression. I say slightly because the true relationship is linear and we are working from a sample of this data, so the cubic regression would likely have fit something close to a linear relationship (it’s not going to fit a wild cubic relationship if the data mostly follow a straight line).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_linear, test_size=0.3, random_state=1)\n",
    "\n",
    "# Linear model\n",
    "lin.fit(X_train, y_train)\n",
    "y_test_pred_lin = lin.predict(X_test)\n",
    "rss_test_lin = np.sum((y_test - y_test_pred_lin)**2)\n",
    "\n",
    "# Cubic model\n",
    "X_train_c = poly.fit_transform(X_train)\n",
    "X_test_c = poly.transform(X_test)\n",
    "cub.fit(X_train_c, y_train)\n",
    "y_test_pred_cub = cub.predict(X_test_c)\n",
    "rss_test_cub = np.sum((y_test - y_test_pred_cub)**2)\n",
    "\n",
    "print(\"=== (b) Linear True — TEST RSS ===\")\n",
    "print(f\"Linear RSS: {rss_test_lin:.2f}\")\n",
    "print(f\"Cubic RSS : {rss_test_cub:.2f}\")\n",
    "print(\"Justification:\")\n",
    "print(\"Here I would expect the linear regression to perform slightly better than the cubic regression. \"\n",
    "      \"I say slightly because the true relationship is linear and we are working from a sample of this data, \"\n",
    "      \"so the cubic regression would likely have fit something close to a linear relationship \"\n",
    "      \"(it’s not going to fit a wild cubic relationship if the data mostly follow a straight line).\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154dde21-1b22-42d5-828b-5116af5a5383",
   "metadata": {},
   "source": [
    "(c) Suppose that the true relationship between X and Y is not linear,\n",
    "but we don’t know how far it is from linear. Consider the training\n",
    "RSS for the linear regression, and also the training RSS for the\n",
    "cubic regression. Would we expect one to be lower than the\n",
    "other, would we expect them to be the same, or is there not\n",
    "enough information to tell? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee44c1b0-22d1-4857-ba43-8aad2984e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== (c) Non-linear True — TRAINING RSS ===\n",
      "Linear RSS: 1353.45\n",
      "Cubic RSS : 364.90\n",
      "Justification:\n",
      "As in part (a), I expect the cubic regression to have a lower training RSS.\n",
      "If the relationship is very non-linear and can be well approximated by a cubic fit, I would expect the training RSS to be much lower.\n",
      "If, however, the relationship is very close to linear or cannot be well approximated by a cubic fit, we might expect more similar results.\n",
      "Either way, the cubic fit would outperform the linear fit, as increasing model flexibility will lead to a reduction in training error.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate non-linear data\n",
    "y_nl = 2 + X.flatten()**2 + np.random.normal(0, 2, n)\n",
    "\n",
    "# Linear model\n",
    "lin.fit(X, y_nl)\n",
    "y_train_pred_lin_nl = lin.predict(X)\n",
    "rss_train_lin_nl = np.sum((y_nl - y_train_pred_lin_nl)**2)\n",
    "\n",
    "# Cubic model\n",
    "X_cubic = poly.fit_transform(X)\n",
    "cub.fit(X_cubic, y_nl)\n",
    "y_train_pred_cub_nl = cub.predict(X_cubic)\n",
    "rss_train_cub_nl = np.sum((y_nl - y_train_pred_cub_nl)**2)\n",
    "\n",
    "print(\"=== (c) Non-linear True — TRAINING RSS ===\")\n",
    "print(f\"Linear RSS: {rss_train_lin_nl:.2f}\")\n",
    "print(f\"Cubic RSS : {rss_train_cub_nl:.2f}\")\n",
    "print(\"Justification:\")\n",
    "print(\"As in part (a), I expect the cubic regression to have a lower training RSS.\\n\"\n",
    "      \"If the relationship is very non-linear and can be well approximated by a cubic fit, \"\n",
    "      \"I would expect the training RSS to be much lower.\\n\"\n",
    "      \"If, however, the relationship is very close to linear or cannot be well approximated by a cubic fit, \"\n",
    "      \"we might expect more similar results.\\n\"\n",
    "      \"Either way, the cubic fit would outperform the linear fit, as increasing model flexibility \"\n",
    "      \"will lead to a reduction in training error.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9e8e3-168e-4e0b-95fb-61158880b854",
   "metadata": {},
   "source": [
    "(d) Answer (c) using test rather than training RSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c3b1ae8-c0ec-46fa-91f5-551e86acf0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== (d) Non-linear True — TEST RSS ===\n",
      "Linear RSS: 479.23\n",
      "Cubic RSS : 132.77\n",
      "Justification:\n",
      "It depends on what the nature of the non-linear relationship is. For most non-linear relationships I would expect the cubic regression to have a lower test RSS, but at the same time, a linear model may give a very good approximation for a mildly non-linear relationship.\n",
      "I think this completely depends. If the relationship is closer to linear, we would expect the linear regression to have a lower test RSS. In the case of a stronger non-linear relationship that is closer to cubic, we would obviously expect the cubic regression to have a lower test RSS.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split non-linear data into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_nl, test_size=0.3, random_state=1)\n",
    "\n",
    "# Linear model\n",
    "lin.fit(X_train, y_train)\n",
    "y_test_pred_lin_nl = lin.predict(X_test)\n",
    "rss_test_lin_nl = np.sum((y_test - y_test_pred_lin_nl)**2)\n",
    "\n",
    "# Cubic model\n",
    "X_train_c = poly.fit_transform(X_train)\n",
    "X_test_c = poly.transform(X_test)\n",
    "cub.fit(X_train_c, y_train)\n",
    "y_test_pred_cub_nl = cub.predict(X_test_c)\n",
    "rss_test_cub_nl = np.sum((y_test - y_test_pred_cub_nl)**2)\n",
    "\n",
    "print(\"=== (d) Non-linear True — TEST RSS ===\")\n",
    "print(f\"Linear RSS: {rss_test_lin_nl:.2f}\")\n",
    "print(f\"Cubic RSS : {rss_test_cub_nl:.2f}\")\n",
    "print(\"Justification:\")\n",
    "print(\"It depends on what the nature of the non-linear relationship is. \"\n",
    "      \"For most non-linear relationships I would expect the cubic regression to have a lower test RSS, \"\n",
    "      \"but at the same time, a linear model may give a very good approximation for a mildly non-linear relationship.\\n\"\n",
    "      \"I think this completely depends. If the relationship is closer to linear, we would expect the linear regression \"\n",
    "      \"to have a lower test RSS. In the case of a stronger non-linear relationship that is closer to cubic, \"\n",
    "      \"we would obviously expect the cubic regression to have a lower test RSS.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
